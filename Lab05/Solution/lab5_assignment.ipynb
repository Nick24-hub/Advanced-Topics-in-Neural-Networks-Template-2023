{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ec6f21f",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/Nick24-hub/Advanced-Topics-in-Neural-Networks-Template-2023/blob/main/Lab05/Solution/lab5_assignment.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3eaf43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import v2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3093f706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "        # For multi-gpu workstations, PyTorch will use the first available GPU (cuda:0), unless specified otherwise\n",
    "        # (cuda:1).\n",
    "    if torch.backends.mps.is_available():\n",
    "        return torch.device('mos')\n",
    "    return torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "525e7bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_size, hidden_size1)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_size1)\n",
    "        self.fc2 = torch.nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_size2)\n",
    "        self.fc3 = torch.nn.Linear(hidden_size2, output_size)\n",
    "        self.relu = torch.nn.ReLU(inplace=True)\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #return self.fc3(self.dropout(self.relu(self.bn2(self.fc2(self.dropout(self.relu(self.bn1(self.fc1(x)))))))))\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1c22d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, labels):\n",
    "    fp_plus_fn = torch.logical_not(output == labels).sum().item()\n",
    "    all_elements = len(output)\n",
    "    return (all_elements - fp_plus_fn) / all_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3ba3033a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "\n",
    "    all_outputs = []\n",
    "    all_labels = []\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch, batch_data in enumerate(train_loader, 0):\n",
    "        data, labels = batch_data;\n",
    "        data = data.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "        output = model(data)\n",
    "        loss = criterion(output, labels)\n",
    "        epoch_loss += loss.item()\n",
    "        writer.add_scalar(\"AdaGrad/Train/Batch loss\", loss.item(), batch)\n",
    "    \n",
    "        loss.backward()\n",
    "\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        output = output.softmax(dim=1).detach().cpu().squeeze()\n",
    "        labels = labels.cpu().squeeze()\n",
    "        all_outputs.append(output)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    all_outputs = torch.cat(all_outputs).argmax(dim=1)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    return round(accuracy(all_outputs, all_labels), 4), epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "53fd4d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(model, val_loader, device):\n",
    "    model.eval()\n",
    "\n",
    "    all_outputs = []\n",
    "    all_labels = []\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for data, labels in val_loader:\n",
    "        data = data.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(data)\n",
    "            \n",
    "        loss = criterion(output, labels)\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        output = output.softmax(dim=1).cpu().squeeze()\n",
    "        labels = labels.cpu().squeeze()\n",
    "        all_outputs.append(output)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    all_outputs = torch.cat(all_outputs).argmax(dim=1)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    \n",
    "    return round(accuracy(all_outputs, all_labels), 4), epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e8bb686d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_epoch(model, train_loader, val_loader, criterion, optimizer, device):\n",
    "    acc, loss = train(model, train_loader, criterion, optimizer, device)\n",
    "    acc_val, loss_val = val(model, val_loader, device)\n",
    "    #torch.cuda.empty_cache()\n",
    "    return acc, acc_val, loss, loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2c684ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_norm(model):\n",
    "    norm = 0.0\n",
    "    for param in model.parameters():\n",
    "        norm += torch.norm(param)\n",
    "    return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "be06f543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "device=get_default_device()\n",
    "print(device)\n",
    "\n",
    "transforms_test = [\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Resize((28, 28), antialias=True),\n",
    "    v2.Grayscale(),\n",
    "    torch.flatten,\n",
    "]\n",
    "\n",
    "transforms_train = [\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Resize((28, 28), antialias=True),\n",
    "    v2.Grayscale(),\n",
    "    v2.RandAugment(),\n",
    "    torch.flatten,\n",
    "]\n",
    "\n",
    "data_path = './data'\n",
    "train_dataset = CIFAR10(root=data_path, train=True, transform=v2.Compose(transforms_train), download=True)\n",
    "val_dataset = CIFAR10(root=data_path, train=False, transform=v2.Compose(transforms_test), download=True)\n",
    "\n",
    "model = MLP(784, 200, 50, 10)\n",
    "model = model.to(device)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "#optimizer = torch.optim.Adam(model.parameters())\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b157efd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "batch_size = 256\n",
    "val_batch_size = 500\n",
    "num_workers = 2\n",
    "persistent_workers = (num_workers != 0)\n",
    "pin_memory = device.type == 'cuda'\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, pin_memory=pin_memory, num_workers=num_workers,\n",
    "                          batch_size=batch_size, drop_last=True, persistent_workers=persistent_workers)\n",
    "val_loader = DataLoader(val_dataset, shuffle=False, pin_memory=True, num_workers=0, batch_size=val_batch_size,\n",
    "                        drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4164b81",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-92f74bc2a207c615\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-92f74bc2a207c615\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir='./runs' --port 6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "644449be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 50/50 [13:10<00:00, 15.80s/it, Acc: 0.394, Acc_val: 0.4502]\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter()\n",
    "tbar = tqdm(tuple(range(epochs)))\n",
    "for epoch in tbar:\n",
    "    acc, acc_val, loss, loss_val = do_epoch(model, train_loader, val_loader, criterion, optimizer, device)\n",
    "    current_learning_rate = optimizer.param_groups[0]['lr']\n",
    "    tbar.set_postfix_str(f\"Acc: {acc}, Acc_val: {acc_val}\")\n",
    "    writer.add_scalar(\"AdaGrad/Train/Accuracy\", acc, epoch)\n",
    "    writer.add_scalar(\"AdaGrad/Train/Loss\", loss, epoch)\n",
    "    writer.add_scalar(\"AdaGrad/Val/Accuracy\", acc_val, epoch)\n",
    "    writer.add_scalar(\"AdaGrad/Val/Loss\", loss_val, epoch)\n",
    "    writer.add_scalar(\"AdaGrad/Model/Norm\", get_model_norm(model), epoch)\n",
    "    writer.add_scalar(\"AdaGrad/Optimizer/Learning rate\", current_learning_rate, epoch)\n",
    "    writer.add_scalar(\"AdaGrad/Optimizer/Batch size\", batch_size, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cb401f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
